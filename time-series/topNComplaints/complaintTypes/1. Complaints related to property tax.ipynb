{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References: \n",
    "1. https://www.otexts.org/fpp/6\n",
    "2. http://cs.wellesley.edu/~cs315/Papers/stl%20statistical%20model.pdf\n",
    "3. http://www.statosphere.com.au/check-time-series-stationary-r/\n",
    "4. https://www.otexts.org/fpp/2/4\n",
    "5. https://www.quora.com/What-is-the-difference-between-autocorrelation-Partial-autocorrelation-and-Inverse-autocorrelation-while-modelling-an-ARIMA-series\n",
    "6. http://people.duke.edu/~rnau/411arim2.htm\n",
    "7. http://people.duke.edu/~rnau/411arim3.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "- In a STL decomposition, how do I pick the window size and the season size?\n",
    "- Is there a scientific process to determine outliers in Time Series Modeling?\n",
    "\n",
    "\n",
    "### TODOs\n",
    "- Understand how the the portmantaeu test, and what parameters to use for it. What p-values are acceptable?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loadData <- function(dataFolder) {\n",
    "    files <- list.files(dataFolder)\n",
    "    data <- list()\n",
    "    for(file in files) {    \n",
    "        df <- read.csv(paste0(dataFolder, \"/\", file), stringsAsFactors=F)    \n",
    "        minYear <- min(df$Year)\n",
    "        complaintType <- substr(file,1,(nchar(file))-4)    \n",
    "        tsObject <- ts(df$Complaints, start=c(minYear, 1), frequency = 12)\n",
    "        data[[complaintType]] <- tsObject\n",
    "    }\n",
    "    data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data <- loadData(\"../../data/topNComplaints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "series <- data[[\"Complaints related to property tax \"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tsdisplay(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data before 2012 are too few to consider\n",
    "series <- window(series, start=c(2012, 1), end=c(2016, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tsdisplay(series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning up data\n",
    "Data can contain outliers, missing values, etc that can hamper the effectiveness of modeling it. \n",
    "Fortunately, the eGovs dataset does not have any missing datapoints, so we really only have to worry about outliers. \n",
    "\n",
    "Outliers can be removed using the `tsclean` method, that comes with the `forecast` module. \n",
    "**Note that the decision to remove / 'cleanse' (winsorizing) outliers should be taken after considerable thought**: Removing outliers has multiple effects - changing the values themselves, the confidence intervals, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a side-by-side plot\n",
    "plot(series, col=\"red\", lty=2)\n",
    "lines(tsclean(series), lty=1)\n",
    "legend(\"topright\", col=c(\"red\", \"black\"), lty=c(2,1), legend=c(\"Original\", \"Cleaned\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plot, since the data exhibits a downtrend in the number of complaints, the 'outliers' that are dampened in 2013-2014 are actually not outliers at all. So in this case, cleansing data is unwarranted.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with decomposition\n",
    "Aim: To decompose the series into 3 components: Trend-Cycle, Seasonal and Remainder components. \n",
    "\n",
    "\n",
    "Methods used:\n",
    "\n",
    "- Simple moving average decomposition for finding trends\n",
    "- Sequention moving averages\n",
    "- Classical Decompsition - Additive and Multiplicative (using methods described above)\n",
    "- STL Decomposition\n",
    "\n",
    "Finally, compute MAPE / RMSE after experimenting to evaluate effectiveness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding trends using moving averages\n",
    "Parmeter to select: order = how many values (left+right) should be picked for computing the average. The higher this is, the smoother the curve. Odd orders are 'symmetric', whereas even orders aren't.  \n",
    "\n",
    "Notation: 4-MA means a MA of order 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fit <- ma(series, order=3)\n",
    "plot(series, main= \"Complaints related to property tax\",\n",
    "     xlab=\"Time\", ylab=\"Number of complaints\", col=\"grey\")\n",
    "lines(fit, col=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear above that a higher order needs to be used for picking out the trend, since the MA curve stil retains some of the peaks/ drops in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "old.par <- par(mfrow=c(4, 2), mar = c(2,2,2,2))\n",
    "for( i in seq(3, 17, 2)) {\n",
    "    fit <- ma(series, order=i)\n",
    "    plot(series, main=paste0(i, \"-MA of Complaints related to property tax\"),\n",
    "         xlab=\"Time\", ylab=\"Number of complaints\", col=\"grey\")\n",
    "    lines(fit, col=\"red\")\n",
    "}\n",
    "\n",
    "par(old.par)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the order smoothes out the curve, but decreases its length. MA smoothing therefore isn't really ideal for this particular method, but gives a good intuition about how trends can be discovered in the data. \n",
    "One other drawback is that only odd orders are symmetric - even orders take an extra value from the left or right - which doesn't make sense most of the times. \n",
    "In the next section, we will take a look at moving averages of moving averages. As mentioned in [1], it can be used to make even order MA symmetric\n",
    "\n",
    "### Moving averages of moving averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# example of a 2 MA of a 4 MA, contrasted with a 4-MA\n",
    "plot(series, main= \"Complaints related to property tax\",\n",
    "     xlab=\"Time\", ylab=\"Number of complaints\", col=\"grey\")\n",
    "lines(ma(ma(series, 4), 2), col=\"red\")\n",
    "lines(ma(series, 4), col=\"blue\")\n",
    "legend('topleft', legend=c(\"2x4-MA\", \"4-MA\"), col=c(\"red\", \"blue\"), lty=c(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's try different 2xm-MAs to see which is appropriate\n",
    "old.par <- par(mfrow=c(4, 2), mar = c(2,2,2,2))\n",
    "for( i in seq(2, 16, 2)) {\n",
    "    fit <- ma(ma(series, i), 2)    \n",
    "    plot(series, main=paste0(\"2x\", i, \"-MA of Complaints related to property tax\"),\n",
    "         xlab=\"Time\", ylab=\"Number of complaints\", col=\"grey\")\n",
    "    lines(fit, col=\"red\")\n",
    "}\n",
    "par(old.par)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like 2x12MA is the smoothest in this series!\n",
    "As is evident in the graph above, taking multiple MAs of the series tends to give much smoother curves. But it still suffers from the problems mentioned for moving averages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical decomposition\n",
    "There are two forms of classical decomposition: an additive decomposition and a multiplicative decomposition. \n",
    "The trend-cycle component is extracted using the methods described above. This is subtracted from the series (or divided for multiplicative decomposition) to obtain the de-trended data. To obtain the seasonal component, the values are simple averaged over the time period. For instance if the time period is 12, a monthly average is taken, if it is 4, a quarterly average is taken. \n",
    "The remainder component is calculated by subtracting (or divining for multiplicative decomposition) the estimated seasonal and trend-cycle components.\n",
    "\n",
    "- As is evident, this method has the disadvantages that come with computing MAs mentioned earlier. \n",
    "- In addition to this, Classical decomposition methods assume that the seasonal component repeats from year to year. This may not be true. There might be a changing seasonal trend over the years, for instance.\n",
    "- Occasionally, the value of the time series in a small number of periods may be particularly unusual. The classical method is not robust to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# additive decomposition - note that the extreme values are missing\n",
    "plot(decompose(series, type=\"additive\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# multiplicative decomposition\n",
    "plot(decompose(series, type=\"multiplicative\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STL decomposition\n",
    "Stands for \"Seasonal and Trend decomposition using Loess\". It is a additive decomposition method. Since it is a fairly complex method of decomposition, details are left out. Details can be found in [2]. \n",
    "It comes with a couple of advantages over the earlier methods. It does handle values near the beginning and end, unlike MA and classical methods, and also accounts for change in seasonal patterns. \n",
    "\n",
    "Couple of disadvantages are: It does not automatically handle trading day or calendar variation, and it only provides facilities for additive decompositions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# need to specify at least s.window, and \n",
    "# optionally two other parameters, t.window and robust (these are the most important, there are others)\n",
    "# s.window -> seasonal window, t.window -> trend window, \n",
    "# these control how rapidly the trend and seasonal components can change. Small values allow more rapid change\n",
    "# you can set s.window to periodic to specify that the seasonal patterns do not change\n",
    "plot(stl(series, s.window=\"periodic\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if you set s.window to smaller values, you can see that the seasonal pattern changes \n",
    "old.par <- par(mfrow=c(2, 2), mar=c(3,3,3,3))\n",
    "plot(stl(series, s.window=3)$time.series[, 1], main=\"Seasonal Component with s.window = 3\")\n",
    "plot(stl(series, s.window=6)$time.series[, 1], main=\"Seasonal Component with s.window = 6\")\n",
    "plot(stl(series, s.window=10)$time.series[, 1], main=\"Seasonal Component with s.window = 10\")\n",
    "plot(stl(series, s.window=12)$time.series[, 1], main=\"Seasonal Component with s.window = 12\")\n",
    "par(old.par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# changing t.window will change the 'smoothness' of the trend curve\n",
    "# it is similar to order in MA trends\n",
    "s.window <- \"periodic\"\n",
    "old.par <- par(mfrow=c(2, 3))\n",
    "for(tw in c(1, 3, 5, 6, 12, 14)) {\n",
    "    plot(stl(series, s.window=s.window, t.window=tw)$time.series[, 2], \n",
    "         main=paste0(\"Trend Component with t.window = \", tw))\n",
    "}\n",
    "par(old.par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO robust - how does it affect the modeling? \n",
    "# TODO documentation only mentions that it affects loess \n",
    "plot(stl(series, s.window=\"periodic\", robust=T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecasting using STL \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stlm = stl + time-series model\n",
    "# by default uses ets, can specify other models\n",
    "startDate <- c(2015, 6)\n",
    "\n",
    "train <- window(series, end=startDate)\n",
    "test <- window(series, start=startDate)\n",
    "fit <- stlm(train, s.window=12, method=\"ets\")\n",
    "prediction <- forecast(fit, 12)\n",
    "plot(prediction, col=\"grey\")\n",
    "lines(test, col=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute MAPEs\n",
    "accuracy(prediction, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA Models\n",
    "\n",
    "### Preprocessing for ARIMA models\n",
    "\n",
    "To fit an ARIMA model, we need to first ensure that the data have some properties. These properties are:\n",
    "- The time series must be stationary\n",
    "\n",
    "### Stationary Time Series\n",
    "# TODO add more details here\n",
    "A stationary time series is one whose properties do not depend on the time at which the series is observed: it will have no predictable patterns in the long-term, in general. Time plots will show the series to be roughly horizontal (although some cyclic behaviour is possible) with constant variance.\n",
    "\n",
    "[3] details several methods to check for stationarity. They are reproduced towards the end. \n",
    "\n",
    "There are certain pre-processing steps one can use to induce stationarity in a non-stationary series. These include using a Box-Cox transform (typically with $\\lambda=0$ i.e log transform), differencing, or a combination of these steps. \n",
    "\n",
    "**Transformations such as logarithms can help to stabilize the variance of a time series. Differencing can help stabilize the mean of a time series by removing changes in the level of a time series, and so eliminating trend and seasonality.**\n",
    "\n",
    "#### Box-Cox transforms\n",
    "This transform is used if there isn't constant variance in the data i.e the data varies (significantly) with time. Box-Cox transforms are detailed in [4]. The parameter to select is $\\lambda$. If $\\lambda=0$, then the transformation is equvalent to a log transform - which is probably the most common form. \n",
    "Note that the `forecast` module comes with this transform inherent in almost every modeling algorithm it has, so a explicit transformation is not needed, one only needs to supply a lambda parameter during modeling. Having said that, it is always a good idea to visualize the transformed data to ensure that the transformation has the intended effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transformed <- BoxCox(series, lambda=0)\n",
    "plot(series)\n",
    "plot(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# try multiple values of lambda\n",
    "old.par <- par(mfrow=c(3, 2), mar = c(2,2,2,2))\n",
    "for( i in c(0, 0.1, 0.25, 0.5, 0.75, 1)) {\n",
    "    transformed <- BoxCox(series, lambda=i)\n",
    "    plot(transformed, main=paste0('lambda=', i),\n",
    "         xlab=\"Time\", ylab=\"Number of complaints\")    \n",
    "}\n",
    "par(old.par)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Differencing data\n",
    "Differencing: compute the differences between consecutive observations.\n",
    "\n",
    "As well as looking at the time plot of the data, the ACF plot is also useful for identifying non-stationary time series. For a stationary time series, the ACF will drop to zero relatively quickly, while the ACF of non-stationary data decreases slowly. Also, for non-stationary data, the value of r1 (first lag) is often large and positive.\n",
    "\n",
    "`lag` indicates which lag to use. By default, it is `1`. \n",
    "`differences` indicates the order of the difference - the number of times the differencing operation needs to happen. By defualt this is 1, but sometimes this is set to 2 or 3 because the data is non-stationary even after first-order differencing. \n",
    "\n",
    "The lag parameter can be set to values other than 1 if seasonal differencing is needed. This is set to the season period. For instance if the seasonal pattern spans a year, then for monthly data, we can get a seasonal differenced data by setting this to 12. Note that ARIMA models require non-seasonal data, so this needs to be carried out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# simple, first-order differencing\n",
    "plot(diff(series, lag=1, differences=1))\n",
    "lines(series, col=\"grey\")\n",
    "legend(\"topright\", legend=c(\"Original\", \"Differenced\"), col=c(\"grey\", \"black\"), lty=c(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# you can use the ndiffs method to get the number of differences required to achieve stationarity\n",
    "ndiffs(series) # set this to param - differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first order seasonal difference\n",
    "plot(diff(series, differences=1, lag=7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACF\n",
    "Before we move on to actual ARIMA, we need to understand what ACF(Autocorrelation Function) is, and how to interpret it. ACF can be a useful tool to identify the tentative parameters for ARIMA. While ACF plots help in understanding the time series in general, these plots are particularly useful for ARIMA, as we will see. Autocorrelation is correlation of a series with itself, with a lag of 1 or more. \n",
    "To compute it, use the `Acf` function (not the `acf` function)\n",
    "\n",
    "Whether a correlation is significant or not can be ascertained by looking at the blue line. If the value is positive and below the blue line or negative and above the blue line, it isn't significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Acf(series, lag.max=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# some additional Acf plots\n",
    "transformed.series <- diff(series, differences=1, lag=13)\n",
    "Acf(transformed.series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(transformed.series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PACF\n",
    "(from [7]):\n",
    "> In general, the \"partial\" correlation between two variables is the amount of correlation between them which is not explained by their mutual correlations with a specified set of other variables. For example, if we are regressing a variable Y on other variables X1, X2, and X3, the partial correlation between Y and X3 is the amount of correlation between Y and X3 that is not explained by their common correlations with X1 and X2. This partial correlation can be computed as the square root of the reduction in variance that is achieved by adding X3 to the regression of Y on X1 and X2. \n",
    "\n",
    "Partial auto-correlation function (PACF) is very similar to ACF. The difference is described in [5]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Pacf(series, lag.max=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AR Models\n",
    "A Autoregressive(AR) model assumes that the series is a function of $p$ prior values in the series - an $AR(p)$ model. \n",
    "To fit a AR model, use the `Arima` function, with the first value in the $order$ being $p$, and the rest 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fit <- Arima(series, order=c(1, 0, 0))\n",
    "plot(forecast(fit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MA models\n",
    "Rather than use past values of the forecast variable in a regression, a moving average model uses past forecast errors in a regression-like model. The number of forecast errors is denoted by $q$, and the model is called a $MA(q)$ model. To fit a MA model, use the `Arima` function, with the third value in the $order$ being $q$, and the rest 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fit <- Arima(series, order=c(0, 0, 12))\n",
    "plot(forecast(fit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA \n",
    "A ARIMA model - Autogressive integrated moving average - is a combination of a $AR(p)$ and $MA(q)$ model, in which the data is differenced $d$ times. These 3 parameters - $(p, d, q)$ (in that order) denote an ARIMA model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## simple model with AR(1), MA(1) and 0 differences\n",
    "fit <- Arima(series, order=c(1, 0, 1))\n",
    "plot(forecast(fit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating $(p, d, q)$\n",
    "\n",
    "These methods are described in detail in [6] and [7]. The following section is a summary of that, along with supporting code.\n",
    "\n",
    "#### Estimating $d$\n",
    "Estimating d, the order of differencing, is described in detail in [6]. An excerpt:\n",
    "> The first (and most important) step in fitting an ARIMA model is the determination of the order of differencing needed to stationarize the series. Normally, the correct amount of differencing is the lowest order of differencing that yields a time series which fluctuates around a well-defined mean value and whose autocorrelation function (ACF) plot decays fairly rapidly to zero, either from above or below. If the series still exhibits a long-term trend, or otherwise lacks a tendency to return to its mean value, or if its autocorrelations are are positive out to a high number of lags (e.g., 10 or more), then it needs a higher order of differencing. \n",
    "\n",
    ">> **Rule 1**: If the series has positive autocorrelations out to a high number of lags, then it probably needs a higher order of differencing.\n",
    "\n",
    ">> **Rule 2**: If the lag-1 autocorrelation is zero or negative, or the autocorrelations are all small and patternless, then the series does not need a higher order of  differencing. If the lag-1 autocorrelation is -0.5 or more negative, the series may be overdifferenced.  **Beware of overdifferencing**!!\n",
    "\n",
    ">> **Rule 3**: The optimal order of differencing is often the order of differencing at which the standard deviation is lowest.\n",
    "\n",
    ">> **Rule 4**: A model with no orders of differencing assumes that the original series is stationary (mean-reverting). A model with one order of differencing assumes that the original series has a constant average trend (e.g. a random walk or SES-type model, with or without growth). A model with two orders of total differencing assumes that the original series has a time-varying trend (e.g. a random trend or LES-type model).\n",
    "\n",
    ">> **Rule 5**: A model with no orders of differencing normally includes a constant term (which allows for a non-zero mean value). A model with two orders of total differencing normally does not include a constant term. In a model with one order of total differencing, a constant term should be included if the series has a non-zero average trend.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# revisit the Acf\n",
    "Acf(series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Acf does not rapidly decay to zero - this may benefit from differencing. But before that, let's visualize the mean and compute the standard deviation. A moving average can also be used to visualize the variation (the higher the order, the closer it gets to the actual mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(series, col=\"grey\")\n",
    "# visualize the mean\n",
    "series.mean <- mean(series)\n",
    "# a 2X4 MA\n",
    "lines(ma(ma(series, order=2), order=4))\n",
    "abline(series.mean, 0, col=\"blue\", lty=2)\n",
    "mtext(paste0(\"SD: \", sd(series)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# difference the series, and visualize it \n",
    "d.series <- diff(series, lag=1, differences = 1)\n",
    "plot(d.series)\n",
    "abline(series.mean, 0, col=\"blue\", lty=2)\n",
    "abline(mean(d.series), 0, col=\"red\", lty=2)\n",
    "mtext(paste0(\"New SD: \", sd(d.series), \" Original SD:\", sd(series)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Acf(d.series, lag.max=27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the Acf is no longer significant at lag 1, and is patternless. However, the standard deviation has increased, and the series has a significant corelation at lag 13 - which is less than ideal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check second order\n",
    "d2.series <- diff(series, lag=1, differences = 2)\n",
    "plot(d2.series)\n",
    "mtext(paste0(\"New SD: \", sd(d2.series), \" Original SD:\", sd(series)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Acf(d2.series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is clearly overdifferenced (Rule2) - there is a high negative correlation at lag1, and the standard deviation also has increased. So clearly $d<=1$. \n",
    "\n",
    "To confirm, it might be helpful to use a moving average to see how the values fluctuate around the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(d.series, col=\"grey\")\n",
    "# a 2x4 MA\n",
    "lines(ma(ma(d.series, order=2), order=4))\n",
    "abline(mean(d.series), 0, col=\"blue\", lty=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating $p$ and $q$\n",
    "Excerpt from [7]:\n",
    "> After a time series has been stationarized by differencing, the next step in fitting an ARIMA model is to determine whether AR or MA terms are needed to correct any autocorrelation that remains in the differenced series.\n",
    "By looking at the autocorrelation function (ACF) and partial autocorrelation (PACF) plots of the differenced series, you can tentatively identify the numbers of AR and/or MA terms that are needed.\n",
    "\n",
    "> The partial autocorrelations at all lags can be computed by fitting a succession of autoregressive models with increasing numbers of lags. In particular, the partial autocorrelation at lag k is equal to the estimated AR(k) coefficient in an autoregressive model with k terms--i.e., a multiple regression model in which Y is regressed on LAG(Y,1), LAG(Y,2), etc., up to LAG(Y,k). Thus, by mere inspection of the PACF you can determine how many AR terms you need to use to explain the autocorrelation pattern in a time series: if the partial autocorrelation is significant at lag k and not significant at any higher order lags--i.e., if the PACF \"cuts off\" at lag k--then this suggests that you should try fitting an autoregressive model of order k \n",
    "\n",
    ">> Rule 6: If the PACF of the differenced series displays a sharp cutoff and/or the lag-1 autocorrelation is positive--i.e., if the series appears slightly \"underdifferenced\"--then consider adding an AR term to the model. The lag at which the PACF cuts off is the indicated number of AR terms.\n",
    "\n",
    "> An MA signature is commonly associated with negative autocorrelation at lag 1--i.e., it tends to arise in series which are slightly overdifferenced.\n",
    "\n",
    ">> Rule 7: If the ACF of the differenced series displays a sharp cutoff and/or the lag-1 autocorrelation is negative--i.e., if the series appears slightly \"overdifferenced\"--then consider adding an MA term to the model. The lag at which the ACF cuts off is the indicated number of MA terms.\n",
    "\n",
    "In addition, consider these rules:\n",
    "\n",
    ">> Rule 8: It is possible for an AR term and an MA term to cancel each other's effects, so if a mixed AR-MA model seems to fit the data, also try a model with one fewer AR term and one fewer MA term--particularly if the parameter estimates in the original model require more than 10 iterations to converge.\n",
    "\n",
    ">> Rule 9: If there is a unit root in the AR part of the model--i.e., if the sum of the AR coefficients is almost exactly 1--you should reduce the number of AR terms by one and increase the order of differencing by one.\n",
    "\n",
    ">> Rule 10: If there is a unit root in the MA part of the model--i.e., if the sum of the MA coefficients is almost exactly 1--you should reduce the number of MA terms by one and reduce the order of differencing by one.\n",
    "\n",
    ">> Rule 11: If the long-term forecasts appear erratic or unstable, there may be a unit root in the AR or MA coefficients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# determining p - look at Pacf\n",
    "# with d=1\n",
    "Pacf(d.series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Rule 7, this plot is characteristic of a $MA(4)$ process - so let's try fitting that first, and then vary $p$ and $q$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelArima <- function(series, order, h, testData = NULL) {\n",
    "    fit <- Arima(series, order=order)\n",
    "    print(summary(fit))\n",
    "    predictions <- forecast(fit, h)\n",
    "    plot(predictions)\n",
    "    if(!is.null(testData)) {\n",
    "        lines(testData, col=\"red\", lty=2)\n",
    "        print(accuracy(predictions, testData))\n",
    "    }\n",
    "    # check if residuals looklike white noise\n",
    "    Acf(residuals(fit), main=\"Residuals\")\n",
    "    # portmantaeu test\n",
    "    print(Box.test(residuals(fit), lag=24, fitdf=4, type=\"Ljung\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modelArima(series, c(0, 1, 4), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modelArima(series, c(0, 1, 5), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modelArima(series, c(1, 1, 4), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modelArima(series, c(1, 1, 5), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all of the models above have residuals with significant ACF at certain lags, which is not ideal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it al together - ARIMA\n",
    "\n",
    "Steps: \n",
    "- Partition Data into a train and hold-out set\n",
    "- Use STL to decompose the series, and adjust it \n",
    "- If necessary, use a BoxCox transform to fix non-constant variance\n",
    "- Estimate (p, d, q) and fit an Arima model\n",
    "- Verify that residuals are white noice using a Ljung test and taking a look at Acf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stl.fit <- stl(series, s.window=8)\n",
    "series.adj <- seasadj(stl.fit)\n",
    "plot(series.adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "series.train <- window(series.adj, end=c(2015, 6))\n",
    "series.test <- window(series.adj, start=c(2015, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Acf(series.train)\n",
    "# looks like d=0 or d=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# look at Acf if d=1\n",
    "Acf(diff(series.train))\n",
    "# looks like MA(14) or MA(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Pacf(series.train)\n",
    "# looks like AR(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modelArima(series.train, c(12, 0, 12), length(series.test), series.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modelArima(series.train, c(1, 1, 14), length(series.test), series.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modelArima(series.train, c(11, 1, 14), length(series.test), series.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## finaly - try a auto.arima\n",
    "fit <- auto.arima(series.train)\n",
    "predictions <- forecast(fit, length(series.test))\n",
    "plot(predictions)\n",
    "lines(series.test, col=\"red\", lty=2)\n",
    "print(accuracy(predictions, series.test))\n",
    "\n",
    "Acf(residuals(predictions))\n",
    "print(Box.test(residuals(fit), lag=24, fitdf=4, type=\"Ljung\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
